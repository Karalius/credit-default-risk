{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\37069\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hummingbird.ml import convert\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../aggregated_data/train_data.csv')\n",
    "test_data = pd.read_csv('../aggregated_data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID = test_data.pop(\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.copy()\n",
    "y = X.pop(\"TARGET\")\n",
    "train_ID = X.pop(\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_hyper_parameters(sgd_model):\n",
    "    \"\"\"\n",
    "    Performs randomized hyperparameter tuning only for alpha parameters.\n",
    "\n",
    "    Args:\n",
    "        sgd_model ([sklearn.linear_model.SGDClassifier]): SGDClassifier\n",
    "\n",
    "    Returns:\n",
    "        RandomizedSearchCV.best_estimator_: Best parameters for the model\n",
    "    \"\"\"\n",
    "    hyperparams = {\"model__alpha\": np.logspace(-4, 2)}\n",
    "    rscv = RandomizedSearchCV(\n",
    "        sgd_model,\n",
    "        hyperparams,\n",
    "        n_iter=15,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=3,\n",
    "        verbose=0,\n",
    "        n_jobs=1,\n",
    "        random_state=1,\n",
    "    ).fit(X, y)\n",
    "\n",
    "    return rscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(\n",
    "    X: pd.DataFrame, y: pd.Series, model_name: str, *args, **kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Stratified three fold cross validation with 5 splits.\n",
    "    Model_name arguments allows to perform validation on 3 models specified under Args section.\n",
    "\n",
    "    Args:\n",
    "        X ([pd.DataFrame]): training dataframe\n",
    "        y ([pd.Series]): target variable\n",
    "        model_name (str): 'sgd' - SGDclassifier, 'xgboost' - XGBClassifier, 'lgbm' - LGBMClassfier\n",
    "\n",
    "    Returns:\n",
    "        float: ROC AUC score\n",
    "    \"\"\"\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=33)\n",
    "    cv_preds = np.zeros(train_data.shape[0])\n",
    "\n",
    "    for train_indices, cv_indices in stratified_cv.split(X, y):\n",
    "        x_train = X.iloc[train_indices]\n",
    "        y_train = y.iloc[train_indices]\n",
    "        x_cv = X.iloc[cv_indices]\n",
    "        y_cv = y.iloc[cv_indices]\n",
    "\n",
    "        if model_name == \"sgd\":\n",
    "            sgd_pipe.fit(x_train, y_train)\n",
    "            cv_preds[cv_indices] = sgd_pipe.predict_proba(x_cv)[:, 1]\n",
    "\n",
    "        if model_name == \"xgboost\":\n",
    "            xgbc = XGBClassifier(**kwargs)\n",
    "            xgbc.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                eval_set=[(x_cv, y_cv)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200,\n",
    "            )\n",
    "            cv_preds[cv_indices] = xgbc.predict_proba(\n",
    "                x_cv, ntree_limit=xgbc.get_booster().best_ntree_limit\n",
    "            )[:, 1]\n",
    "\n",
    "        if model_name == \"lgbm\":\n",
    "            lgbm_clf = LGBMClassifier(**kwargs)\n",
    "            lgbm_clf.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                eval_set=[(x_cv, y_cv)],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200,\n",
    "            )\n",
    "            cv_preds[cv_indices] = lgbm_clf.predict_proba(\n",
    "                x_cv, num_iteration=lgbm_clf.best_iteration_\n",
    "            )[:, 1]\n",
    "\n",
    "    return round(roc_auc_score(y, cv_preds), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_csv(predictions: np.array, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function converts predictions of a certain estimator to a csv file.\n",
    "\n",
    "    Args:\n",
    "        predictions (np.array): prediction of the target variable preferably on the test data\n",
    "        model_name (str): name of the estimator\n",
    "    \"\"\"\n",
    "    predictions = pd.DataFrame(predictions).iloc[:, 1:]\n",
    "    sub = pd.DataFrame()\n",
    "    sub[\"SK_ID_CURR\"] = test_ID\n",
    "    sub[\"TARGET\"] = predictions\n",
    "    sub.to_csv(f\"{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression by default uses Gradient Descent and as such it would be better to use SGD Classifier on larger data sets. One another reason you might want to use SGD Classifier is, logistic regression, in its vanilla sklearn form, won’t work if you can’t hold the dataset in RAM but SGD will still work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha parameter has been found using sgd_hyper_parameters function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "These scalers have been tried: MinMaxScaler, RobustScaler. However, ```StandardScaler``` showed the best CV score.\n",
    "\n",
    "These imputing options have been tried: IterativeImputer, Median method, Mean method. However, ```zero``` imputing has been giving the best CV score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=0),\n",
    "        ),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"model\",\n",
    "            SGDClassifier(\n",
    "                alpha=0.021209508879201904,\n",
    "                loss=\"log\",\n",
    "                penalty=\"l2\",\n",
    "                class_weight=\"balanced\",\n",
    "                n_jobs=-1,\n",
    "                random_state=0,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7783"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(X, y, 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model = sgd_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 ms ± 24.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sgd_clf_predictions = sgd_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_to_csv(sgd_clf_predictions, 'sgd_clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGD_KAGGLE](https://i.imgur.com/O1lm65H.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baysian optimization XGB\n",
    "\n",
    "For both XGBoost and LightGBM, we have too many hyperparameters to tune, and doing GridSearchCV or RandomizedSearchCV can be too expesive on such a big dataset for finding an optimal solution. That is why we will be using the Bayesian Optimization Technique to tune the hyperparameters, which works by looking at the results on previous hyperparameters while assigning new hyperparameters. It tries to model on the Cost Function which is dependent on all the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluation(\n",
    "    max_depth,\n",
    "    min_child_weight,\n",
    "    gamma,\n",
    "    subsample,\n",
    "    colsample_bytree,\n",
    "    colsample_bylevel,\n",
    "    colsample_bynode,\n",
    "    reg_alpha,\n",
    "    reg_lambda):\n",
    "    \"\"\"\n",
    "    Objective function for Bayesian Optimization of XGBoost's Hyperparamters. Takes the hyperparameters as input, and\n",
    "    returns the Cross-Validation AUC as output.\n",
    "\n",
    "    Inputs: Hyperparamters to be tuned.\n",
    "        max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel,\n",
    "        colsample_bynode, reg_alpha, reg_lambda\n",
    "\n",
    "    Returns:\n",
    "        CV ROC-AUC Score\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"gpu_id\": 0,\n",
    "        \"max_depth\": int(round(max_depth)),\n",
    "        \"min_child_weight\": int(round(min_child_weight)),\n",
    "        \"subsample\": subsample,\n",
    "        \"gamma\": gamma,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"colsample_bylevel\": colsample_bylevel,\n",
    "        \"colsample_bynode\": colsample_bynode,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"random_state\": 1,\n",
    "    }\n",
    "\n",
    "    stratified_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    cv_preds = np.zeros(train_data.shape[0])\n",
    "\n",
    "    # iterating over each fold, training the model, and making Out of Fold Predictions\n",
    "    for train_indices, cv_indices in stratified_cv.split(X, y):\n",
    "\n",
    "        x_train = X.iloc[train_indices]\n",
    "        y_train = y.iloc[train_indices]\n",
    "        x_cv = X.iloc[cv_indices]\n",
    "        y_cv = y.iloc[cv_indices]\n",
    "\n",
    "        xgbc = XGBClassifier(**params)\n",
    "        xgbc.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            eval_set=[(x_cv, y_cv)],\n",
    "            eval_metric=\"auc\",\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "        cv_preds[cv_indices] = xgbc.predict_proba(\n",
    "            x_cv, ntree_limit=xgbc.get_booster().best_ntree_limit\n",
    "        )[:, 1]\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(y, cv_preds)\n",
    "\n",
    "\n",
    "bopt_xgb = BayesianOptimization(\n",
    "    xgb_evaluation,\n",
    "    {\n",
    "        \"max_depth\": (5, 15),\n",
    "        \"min_child_weight\": (5, 80),\n",
    "        \"gamma\": (0.2, 1),\n",
    "        \"subsample\": (0.5, 1),\n",
    "        \"colsample_bytree\": (0.5, 1),\n",
    "        \"colsample_bylevel\": (0.3, 1),\n",
    "        \"colsample_bynode\": (0.3, 1),\n",
    "        \"reg_alpha\": (0.001, 0.3),\n",
    "        \"reg_lambda\": (0.001, 0.3),\n",
    "    },\n",
    "    random_state=1,\n",
    ").maximize(n_iter=6, init_points=4)\n",
    "\n",
    "\n",
    "target_values = []\n",
    "for result in bopt_xgb.res:\n",
    "    target_values.append(result[\"target\"])\n",
    "    if result[\"target\"] == max(target_values):\n",
    "        best_params = result[\"params\"]\n",
    "\n",
    "print(\"Best Hyperparameters for XGBoost are:\\n\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters below have been found using bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"gpu_id\": 0,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 80,\n",
    "    \"subsample\": 0.9622896832878278,\n",
    "    \"gamma\": 0.794005454765522,\n",
    "    \"colsample_bytree\": 0.5741523601432443,\n",
    "    \"colsample_bylevel\": 0.3272128085080071,\n",
    "    \"colsample_bynode\": 0.9480907366417157,\n",
    "    \"reg_alpha\": 0.24018946957919934,\n",
    "    \"reg_lambda\": 0.23887141295582165,\n",
    "    \"random_state\": 51412,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7848"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(X, y, 'xgboost', xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:35:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**xgb_params)\n",
    "xgb_model = xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.22 s ± 518 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "xgb_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_to_csv(xgb_predictions, 'xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XGB_KAGGLE](https://i.imgur.com/51FY7P8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baysian optimization LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_evaluation(\n",
    "    num_leaves,\n",
    "    max_depth,\n",
    "    min_split_gain,\n",
    "    min_child_weight,\n",
    "    min_child_samples,\n",
    "    subsample,\n",
    "    colsample_bytree,\n",
    "    reg_alpha,\n",
    "    reg_lambda):\n",
    "    \"\"\"\n",
    "    Objective function for Bayesian Optimization of LightGBM's Hyperparamters. Takes the hyperparameters as input, and\n",
    "    returns the Cross-Validation AUC as output.\n",
    "\n",
    "    Inputs: Hyperparamters to be tuned.\n",
    "        num_leaves, max_depth, min_split_gain, min_child_weight,\n",
    "        min_child_samples, subsample, colsample_bytree, reg_alpha, reg_lambda\n",
    "\n",
    "    Returns:\n",
    "        CV ROC-AUC Score\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"n_jobs\": -1,\n",
    "        \"num_leaves\": int(round(num_leaves)),\n",
    "        \"max_depth\": int(round(max_depth)),\n",
    "        \"min_split_gain\": min_split_gain,\n",
    "        \"min_child_weight\": min_child_weight,\n",
    "        \"min_child_samples\": int(round(min_child_samples)),\n",
    "        \"subsample\": subsample,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 266,\n",
    "    }\n",
    "    stratified_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=33)\n",
    "\n",
    "    cv_preds = np.zeros(train_data.shape[0])\n",
    "\n",
    "    for train_indices, cv_indices in stratified_cv.split(X, y):\n",
    "\n",
    "        x_tr = X.iloc[train_indices]\n",
    "        y_tr = y.iloc[train_indices]\n",
    "        x_cv = X.iloc[cv_indices]\n",
    "        y_cv = y.iloc[cv_indices]\n",
    "\n",
    "        lgbm_clf = LGBMClassifier(**params)\n",
    "        lgbm_clf.fit(\n",
    "            x_tr,\n",
    "            y_tr,\n",
    "            eval_set=[(x_cv, y_cv)],\n",
    "            eval_metric=\"auc\",\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "        cv_preds[cv_indices] = lgbm_clf.predict_proba(\n",
    "            x_cv, num_iteration=lgbm_clf.best_iteration_\n",
    "        )[:, 1]\n",
    "\n",
    "    return roc_auc_score(y, cv_preds)\n",
    "\n",
    "\n",
    "bopt_lgbm = BayesianOptimization(\n",
    "    lgbm_evaluation,\n",
    "    {\n",
    "        \"num_leaves\": (25, 50),\n",
    "        \"max_depth\": (6, 11),\n",
    "        \"min_split_gain\": (0, 0.1),\n",
    "        \"min_child_weight\": (5, 80),\n",
    "        \"min_child_samples\": (5, 80),\n",
    "        \"subsample\": (0.5, 1),\n",
    "        \"colsample_bytree\": (0.5, 1),\n",
    "        \"reg_alpha\": (0.001, 0.3),\n",
    "        \"reg_lambda\": (0.001, 0.3),\n",
    "    },\n",
    "    random_state=2,\n",
    ").maximize(n_iter=6, init_points=4)\n",
    "\n",
    "\n",
    "target_values = []\n",
    "for result in bopt_lgbm.res:\n",
    "    target_values.append(result[\"target\"])\n",
    "    if result[\"target\"] == max(target_values):\n",
    "        best_params = result[\"params\"]\n",
    "\n",
    "print(\"Best Hyperparameters obtained are:\\n\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters below have been found using bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_leaves\": 39,\n",
    "    \"max_depth\": 9,\n",
    "    \"min_split_gain\": 0.030820727751758883,\n",
    "    \"min_child_weight\": 30.074868967458226,\n",
    "    \"min_child_samples\": 31,\n",
    "    \"subsample\": 0.7653763123038788,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"colsample_bytree\": 0.6175714684701181,\n",
    "    \"reg_alpha\": 0.15663020002553255,\n",
    "    \"reg_lambda\": 0.22503178038757748,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 266,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7901"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(X, y, 'lgbm', lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(**lgbm_params)\n",
    "lgbm_model = lgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3 s ± 384 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lgbm_predictions = lgbm_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_to_csv(lgbm_predictions, 'lgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LGBM_KAGGLE](https://i.imgur.com/W0Hcz7L.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>CV score</th>\n",
       "      <th>Kaggle</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>0.7611</td>\n",
       "      <td>0.74s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.7848</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>4.22s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.7901</td>\n",
       "      <td>0.7871</td>\n",
       "      <td>21.3s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  CV score  Kaggle  Speed\n",
       "1   SGD    0.7783  0.7611  0.74s\n",
       "2   XGB    0.7848  0.7859  4.22s\n",
       "3  LGBM    0.7901  0.7871  21.3s"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        [\"SGD\", 0.7783, 0.7611, \"0.74s\"],\n",
    "        [\"XGB\", 0.7848, 0.7859, \"4.22s\"],\n",
    "        [\"LGBM\", 0.7901, 0.7871, \"21.3s\"],\n",
    "    ],\n",
    "    columns=[\"Model\", \"CV score\", \"Kaggle\", \"Speed\"],\n",
    "    index=list(range(1, 4)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fastest model - SGDClassifier.\n",
    "2. Most accurate model - LGBMClassifier. Both CV and Kaggle private score.\n",
    "3. XGB and LGBM are better than a median score on Kaggle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase predictions speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase speed of the predictions, I have converted all the models to pytorch back-end with hummingbird-ml library [Github](https://github.com/microsoft/hummingbird).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following results have been achieved:\n",
    "1. SGDClassifier - speed has increased by 6 times.\n",
    "2. XGB - speed has increased by 3 times.\n",
    "3. LGBM - spped has increased by 3 times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since the converted model must run predictions on GPU rather than CPU, it makes it challenging to show on local machine. Thus, I have decided to leave this out of the project scope, however, I found it worth mentioning in case higher prediction speed is required."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1057d4e14222cc9597238fbffe3c18e71fba6b1e3f5a5d1658f88afdef43605"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
